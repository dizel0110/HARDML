{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T08:17:13.972438Z",
     "start_time": "2021-06-17T08:17:13.953124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting solution_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile solution_5.py\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "glue_qqp_dir = 'data/QQP'\n",
    "glove_path = 'data/glove.6B.50d.txt'\n",
    "\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma_ = torch.FloatTensor([self.sigma])\n",
    "        return torch.exp(-torch.square(torch.sub(x, self.mu)) / (2*torch.square(sigma_)))\n",
    "\n",
    "\n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        \n",
    "        start = -1 + 1 / (self.kernel_num - 1)\n",
    "        step = 2 / (self.kernel_num - 1)\n",
    "        for mu in np.arange(start, 1, step):\n",
    "            kernels.append(GaussianKernel(mu, self.sigma))\n",
    "        kernels.append(GaussianKernel(sigma=self.exact_sigma))\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        if len(self.out_layers) == 0:\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.kernel_num, 1)\n",
    "            )\n",
    "        else:            \n",
    "            sizes = [self.kernel_num]\n",
    "            sizes += self.out_layers\n",
    "            sizes += [1]\n",
    "            layers = []\n",
    "            for i in range(len(sizes) - 1):\n",
    "                layers.append(torch.nn.Linear(sizes[i], sizes[i+1]))\n",
    "                layers.append(torch.nn.ReLU())\n",
    "            return torch.nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        q_emb = self.embeddings(query)\n",
    "        d_emb = self.embeddings(doc)\n",
    "               \n",
    "        return torch.einsum('ijk,imk->ijm',\n",
    "                            F.normalize(q_emb, dim=-1),\n",
    "                            F.normalize(d_emb, dim=-1))\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left], [Batch, Right]\n",
    "        query, doc = inputs['query'], inputs['document']                \n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        idxs = [\n",
    "            self.vocab[word]\n",
    "            if self.vocab.get(word) is not None\n",
    "            else self.vocab[self.oov_val]\n",
    "            for word in tokenized_text\n",
    "        ]\n",
    "        return idxs[:self.max_len]\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        text = self.idx_to_text_mapping[idx]\n",
    "        tokens = self.preproc_func(text)\n",
    "        return self._tokenized_text_to_index(tokens)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query, doc1, doc2, score = self.index_pairs_or_triplets[idx]\n",
    "        return (\n",
    "            {\n",
    "                'query': self._convert_text_idx_to_token_idxs(query),\n",
    "                'document': self._convert_text_idx_to_token_idxs(doc1),\n",
    "            },\n",
    "            {\n",
    "                'query': self._convert_text_idx_to_token_idxs(query),\n",
    "                'document': self._convert_text_idx_to_token_idxs(doc2),\n",
    "            },\n",
    "            score,\n",
    "        )\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query, doc, score = self.index_pairs_or_triplets[idx]\n",
    "        return (\n",
    "            {\n",
    "                'query': self._convert_text_idx_to_token_idxs(query),\n",
    "                'document': self._convert_text_idx_to_token_idxs(doc),\n",
    "            },\n",
    "            score,\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        self.train_triplets = self.sample_data_for_train_iter(self.glue_train_df, random_seed)\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "        \n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg, \n",
    "              self.idx_to_text_mapping_dev, \n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "        \n",
    "        self.train_dataset = TrainTripletsDataset(self.train_triplets, \n",
    "              self.idx_to_text_mapping_train, \n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "            collate_fn=collate_fn, shuffle=True)\n",
    "    \n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', error_bad_lines=False, dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "        table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        return inp_str.translate(table)\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        words = nltk.word_tokenize(self.hadle_punctuation(inp_str).lower())\n",
    "        return words\n",
    "    \n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        return {w: n for w, n in vocab.items() if n >= min_occurancies}\n",
    "    \n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        texts = []\n",
    "        for df in list_of_df:\n",
    "            texts += [df['text_left'], df['text_right']]\n",
    "        unique_texts = pd.concat(texts).drop_duplicates()\n",
    "        \n",
    "        words = []\n",
    "        for text in unique_texts.values:\n",
    "            words += self.simple_preproc(text)\n",
    "        counter = Counter(words)\n",
    "        \n",
    "        del words, unique_texts, texts\n",
    "        \n",
    "        counter = self._filter_rare_words(counter, min_occurancies)\n",
    "        counter = list(counter.keys())\n",
    "        return counter\n",
    "            \n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        embeddings = {}\n",
    "        with open(file_path) as file:\n",
    "            for line in file:\n",
    "                items = line.split()\n",
    "                embeddings[items[0]] = items[1:]\n",
    "        return embeddings       \n",
    "        \n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        np.random.seed(random_seed)\n",
    "        embeddings = self._read_glove_embeddings(file_path)\n",
    "        embedding_size = 50\n",
    "        matrix = [\n",
    "            [0.0] * embedding_size,\n",
    "            [1.0] * embedding_size,\n",
    "        ]\n",
    "        vocab = {'PAD': 0, 'OOV': 1, }\n",
    "        unk_words = [\n",
    "            'PAD',\n",
    "            'OOV',\n",
    "        ]\n",
    "        for idx, token in enumerate(inner_keys):\n",
    "            if embeddings.get(token) is not None:\n",
    "                matrix.append(np.array(embeddings[token], dtype=np.float64))\n",
    "            else:\n",
    "                vector = np.random.uniform(low=-self.emb_rand_uni_bound,\n",
    "                                           high=self.emb_rand_uni_bound,\n",
    "                                           size=embedding_size)\n",
    "                matrix.append(vector)\n",
    "                unk_words.append(token)\n",
    "            vocab[token] = idx + 2\n",
    "        assert len(matrix) == len(vocab)\n",
    "        return np.array(matrix), vocab, unk_words\n",
    "                \n",
    "        \n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "    \n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int\n",
    "                                   ) -> List[List[Union[str, float]]]:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        id_left_groups = inp_df.groupby('id_left')\n",
    "        \n",
    "        triplets = []\n",
    "        for id_left, group in id_left_groups:\n",
    "            if group['label'].nunique() > 1:\n",
    "                for label in group['label'].unique():\n",
    "                    ids_same_labels = group[group['label'] == label]['id_right'].values\n",
    "                    ids_less_labels = group[group['label'] < label]['id_right'].values\n",
    "                    \n",
    "                    if len(ids_same_labels) > 1:\n",
    "                        ids_select = np.random.choice(ids_same_labels,\n",
    "                                                      size=2,\n",
    "                                                      replace=False)\n",
    "                        triplets.append([id_left,\n",
    "                                         ids_select[0],\n",
    "                                         ids_select[1],\n",
    "                                         0.5])\n",
    "                    elif len(ids_less_labels) != 0:\n",
    "                        id_pos = np.random.choice(ids_same_labels,\n",
    "                                                  size=1,\n",
    "                                                  replace=False)\n",
    "                        id_neg = np.random.choice(ids_less_labels,\n",
    "                                                  size=1,\n",
    "                                                  replace=False)\n",
    "                        triplets.append([id_left,\n",
    "                                         id_pos[0],\n",
    "                                         id_neg[0],\n",
    "                                         1])\n",
    "        return triplets\n",
    "        \n",
    "\n",
    "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def dcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        order = ys_pred.argsort()[::-1][:ndcg_top_k]\n",
    "        index = np.arange(len(order)) + 1\n",
    "        return ((2**ys_true[order] - 1) / np.log2(index + 1)).sum()\n",
    "    \n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        try:\n",
    "            return self.dcg_k(ys_true, ys_pred, ndcg_top_k) / self.dcg_k(ys_true, ys_true, ndcg_top_k)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "        \n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "        \n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "            \n",
    "        for epoch in range(n_epochs):\n",
    "            for i, batch in enumerate(self.train_dataloader):\n",
    "                inp_1, inp_2, y = batch\n",
    "                \n",
    "                \n",
    "                preds = self.model(inp_1, inp_2)\n",
    "                loss = criterion(preds, y)\n",
    "                \n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ndcg_val = self.valid(self.model,\n",
    "                                      self.val_dataloader)\n",
    "                if ndcg_val >= 0.925:\n",
    "                    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T08:20:15.342100Z",
     "start_time": "2021-06-17T08:17:13.974810Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 4.22 s, total: 3min 1s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import solution_5\n",
    "import importlib\n",
    "importlib.reload(solution_5)\n",
    "\n",
    "s = solution_5.Solution(solution_5.glue_qqp_dir,\n",
    "                        solution_5.glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T08:22:37.365879Z",
     "start_time": "2021-06-17T08:20:15.345251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 19s, sys: 616 ms, total: 4min 20s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T09:48:25.821892Z",
     "start_time": "2021-06-17T09:48:25.575898Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "torch.save(s.model.mlp.state_dict(), 'knrm_mlp.pickle')\n",
    "torch.save(s.model.embeddings.state_dict(), 'embedings.pickle')\n",
    "json.dump(s.vocab,\n",
    "          open('vocab.json', 'w', encoding='utf-8'),\n",
    "          ensure_ascii=False,\n",
    "          indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
